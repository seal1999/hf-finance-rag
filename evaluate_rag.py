"""
RAG ì‹œìŠ¤í…œ í‰ê°€ í”„ë¡œê·¸ë¨
1. ê³ ì •ëœ í…ŒìŠ¤íŠ¸ì…‹ ì‚¬ìš© (ì¼ê´€ëœ ë¹„êµë¥¼ ìœ„í•´)
2. RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ ìƒì„±
3. LLM-as-Judgeë¡œ í‰ê°€ (Faithfulness, Relevancy, Correctness)
"""

import json
import pandas as pd
from datetime import datetime
from langchain_community.llms import Ollama
from hybrid_rag_query import AdvancedHybridRAG

# ê³ ì •ëœ í…ŒìŠ¤íŠ¸ì…‹ (ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•´)
FIXED_TEST_SET = [
    {
        "question": "ì§€ëŠ¥í˜• ë°˜ë„ì²´ íŒ¹ë¦¬ìŠ¤ ê¸°ì—…ì„ ì°¾ì€ ìµœê¸°ì˜ ì¥ê´€ì´ ì–´ë–¤ ì§€ì¹¨ì„ ë°›ì•˜ëŠ”ì§€ ì•Œë ¤ì£¼ì„¸ìš”.",
        "reference": "ìµœê¸°ì˜ ì¥ê´€ì€ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§€ì¹¨ì„ ë°›ì•˜ë‹¤ê³  ì•Œë ¤ì ¸ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."
    },
    {
        "question": "ì—‘ì† ëª¨ë¹Œ ì½”í¼ë ˆì´ì…˜ì˜ ìíšŒì‚¬ë¡œ ì‚¬ìš©ë˜ëŠ” ì´ë¦„ì´ ë¬´ì—‡ì¸ê°€ìš”?",
        "reference": "ì—‘ìŠ¨ ëª¨ë¹Œ ì½”í¼ë ˆì´ì…˜."
    },
    {
        "question": "ìœŒë¦¬ì—„ìŠ¤ ì»´í¼ë‹ˆì¦ˆ ì£¼ì‹íšŒì‚¬ê°€ ìµœê·¼ í™•ì¥í•œ ëª©ì ì€ ë¬´ì—‡ì¸ê°€?",
        "reference": "ìœŒë¦¬ì—„ìŠ¤ ì»´í¼ë‹ˆì¦ˆ ì£¼ì‹íšŒì‚¬ëŠ” ìµœê·¼ í™•ì¥ì„ í†µí•´ ë¹„ì¦ˆë‹ˆìŠ¤ í™œë™ì„ í™•ëŒ€í•˜ê³ , ìƒˆë¡œìš´ ì‹œì¥ì— ì§„ì¶œí•˜ì—¬ ì„±ì¥í•  ìˆ˜ ìˆëŠ” ê¸°íšŒë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤."
    },
    {
        "question": "ê²½ì˜ì§„ì´ í™•ì¸ì„œë¥¼ ë°œê¸‰í•˜ì˜€ì„ ë•Œ, ì´ë€ì— ëŒ€í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì—¬ë¶€ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?",
        "reference": "í™•ì¸ì„œì—ëŠ” ëŒ€í‘œì´ì‚¬ í™•ì¸ì˜ ë‚´ìš©ë§Œ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    },
    {
        "question": "ë°˜ë„ì²´ ì‹œì¥ì˜ ì„±ì¥ì„ ì–´ë–»ê²Œ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?",
        "reference": "ë°˜ë„ì²´ ì‹œì¥ì€ 2020ë…„ ì´í›„ ì „ ì„¸ê³„ì ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì„±ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë°˜ë„ì²´ ê¸°ìˆ ì˜ ë°œì „ê³¼ ëª¨ë°”ì¼, ì»´í“¨í„°, ìë™ì°¨ ë“± ë‹¤ì–‘í•œ ì‚°ì—…ì—ì„œ ë°˜ë„ì²´ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•œ ì¸ì‹ ì¦ê°€ë¡œ ì¸í•´ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    },
    {
        "question": "ì£¼ì‹ ì‹œì¥ì˜ ìƒìŠ¹í­ì´ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì´ë™í–ˆëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "reference": "ì£¼ì‹ ì‹œì¥ì˜ ìƒìŠ¹í­ì€ ì¤„ì–´ë“¤ì—ˆìŠµë‹ˆë‹¤."
    },
    {
        "question": "ë°˜ë„ì²´ ì‹œì¥ì˜ ë‘ ê°€ì§€ ì£¼ìš” ì„¸ê·¸ë¨¼íŠ¸ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?",
        "reference": "ë¡œì§ ë°˜ë„ì²´ì™€ ë©”ëª¨ë¦¬ ë°˜ë„ì²´ë¡œ, ë¡œì§ ë°˜ë„ì²´ëŠ” ë§ˆì´í¬ë¡œí”„ë¡œì„¸ì„œ, ì„¼ì„œ ë° ê¸°íƒ€ ì»´í“¨íŒ… ì¥ì¹˜ì— ì‚¬ìš©ë˜ë©°, ë©”ëª¨ë¦¬ ë°˜ë„ì²´ëŠ” DRAM, SRAM ë° í”Œë˜ì‹œ ë©”ëª¨ë¦¬ì™€ ê°™ì€ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë©”ëª¨ë¦¬ ì¹©ì„ í¬í•¨í•©ë‹ˆë‹¤."
    },
    {
        "question": "ë°˜ë„ì²´ íŒ¨í‚¤ì§• ì†Œì¬ ì‹œì¥ì´ ì„±ì¥í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?",
        "reference": "ë°˜ë„ì²´ì˜ ì§‘ì í™” ë° ì†Œí˜•í™”ê°€ ì§„í–‰ë¨ì— ë”°ë¼ ë°˜ë„ì²´ íŒ¨í‚¤ì§• ì†Œì¬ ì‹œì¥ì´ ì„±ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤."
    },
    {
        "question": "ì›”ìŠ¤íŠ¸ë¦¬íŠ¸ì˜ ê°œë…ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?",
        "reference": "ì›”ìŠ¤íŠ¸ë¦¬íŠ¸ëŠ” ì„¸ê³„ì—ì„œ ê°€ì¥ í° ê¸ˆìœµ ì‹œì¥ì…ë‹ˆë‹¤."
    },
    {
        "question": "AbbVieì˜ ì£¼ê°€ ê°•ì„¸ëŠ” ì–´ë–¤ ìš”ì¸ì— ì˜í•´ ì¸ìƒë˜ë‚˜ìš”?",
        "reference": "AbbVieì˜ ì£¼ê°€ ê°•ì„¸ëŠ” AbbVieì˜ ìƒˆë¡œìš´ ì•½ë¬¼ ê°œë°œê³¼ ì˜ë£Œ ë¶„ì•¼ì˜ ì„±ì¥ potentialì— ì˜í•´ ì¸ìƒë˜ë©°, ì´ë¡œ ì¸í•´ íˆ¬ììë“¤ì˜ ì‹ ë¢°ë¥¼ ì–»ê³  ìˆìŠµë‹ˆë‹¤."
    },
]


def evaluate_answer(question, answer, reference, context, llm):
    """LLM-as-Judgeë¡œ ë‹µë³€ í‰ê°€"""
    prompt = f"""RAG ì‹œìŠ¤í…œì˜ ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}
ì •ë‹µ: {reference}
RAGë‹µë³€: {answer[:500]}
ë¬¸ì„œ: {context[:1000]}

í‰ê°€ ê¸°ì¤€ (1-5ì ):
- faithfulness: ë‹µë³€ì´ ë¬¸ì„œì— ì¶©ì‹¤í•œê°€?
- relevancy: ë‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆí•œê°€?
- correctness: ë‹µë³€ì´ ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ”ê°€?

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥:
{{"faithfulness": ìˆ«ì, "relevancy": ìˆ«ì, "correctness": ìˆ«ì, "comment": "í‰ê°€"}}"""

    response = llm.invoke(prompt)

    try:
        result = response.strip()
        # JSON ê°ì²´ ì°¾ê¸°
        start_idx = result.find("{")
        end_idx = result.rfind("}") + 1
        if start_idx != -1 and end_idx > start_idx:
            result = result[start_idx:end_idx]
        parsed = json.loads(result)
        # ì ìˆ˜ ê²€ì¦ ë° ê¸°ë³¸ê°’
        for key in ["faithfulness", "relevancy", "correctness"]:
            if key not in parsed or not isinstance(parsed[key], (int, float)):
                parsed[key] = 3  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¤‘ê°„ê°’
        return parsed
    except:
        return {"faithfulness": 3, "relevancy": 3, "correctness": 3, "comment": "íŒŒì‹± ì‹¤íŒ¨ - ì¤‘ê°„ê°’ ì ìš©"}


def generate_markdown_report(results, avg_scores, timestamp):
    """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í‰ê°€ ë³´ê³ ì„œ ìƒì„±"""
    overall_avg = sum(avg_scores.values()) / 3

    report = f"""# RAG í‰ê°€ ê²°ê³¼ ë³´ê³ ì„œ

## ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½

| ë©”íŠ¸ë¦­ | í‰ê·  | í•´ì„ |
|--------|------|------|
| Faithfulness | {avg_scores['faithfulness']:.2f} | ë¬¸ì„œ ì¶©ì‹¤ë„ |
| Relevancy | {avg_scores['relevancy']:.2f} | ì§ˆë¬¸ ê´€ë ¨ì„± |
| Correctness | {avg_scores['correctness']:.2f} | ì •ë‹µ ì¼ì¹˜ë„ |
| **ì „ì²´ í‰ê· ** | **{overall_avg:.2f}** | |

### ì ìˆ˜ ê°€ì´ë“œ

- **4.0 ì´ìƒ**: ìš°ìˆ˜ (Production Ready)
- **3.0â€“4.0**: ì–‘í˜¸ (ê°œì„  ì—¬ì§€ ìˆìŒ)
- **3.0 ë¯¸ë§Œ**: ê°œì„  í•„ìš”

---

## ğŸ“‹ ê°œë³„ í‰ê°€ ê²°ê³¼

"""

    for i, r in enumerate(results, 1):
        report += f"""### [{i}] {r['question']}

- **ì •ë‹µ**: {r['reference']}
- **RAG ë‹µë³€**: {r['rag_answer']}
- **ì ìˆ˜**: F={r['faithfulness']}, R={r['relevancy']}, C={r['correctness']}
- **í‰ê°€**: {r['comment'] if r['comment'] else '(í‰ê°€ ì½”ë©˜íŠ¸ ì—†ìŒ)'}

---

"""

    report += f"""## ë©”íŠ¸ë¦­ ì„¤ëª…

- **F (Faithfulness)**: RAG ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€ (í™˜ê° ì—¬ë¶€)
- **R (Relevancy)**: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€
- **C (Correctness)**: ë‹µë³€ì´ ì°¸ì¡° ì •ë‹µê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€

*ìƒì„¸ ìˆ˜ì¹˜ ê²°ê³¼ëŠ” `evaluation_results_{timestamp}.csv`ì— ì €ì¥ë¨.*
"""

    return report


def run_evaluation():
    """ì „ì²´ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    print("=" * 60)
    print("ğŸ§ª RAG ì‹œìŠ¤í…œ í‰ê°€ ì‹œì‘ (ê³ ì • í…ŒìŠ¤íŠ¸ì…‹)")
    print(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    # 1. LLM ì´ˆê¸°í™”
    print("\n[0/3] LLM ì´ˆê¸°í™” ì¤‘...")
    llm = Ollama(model="llama3.2", temperature=0.3)

    # 2. ê³ ì • í…ŒìŠ¤íŠ¸ì…‹ ì‚¬ìš©
    test_df = pd.DataFrame(FIXED_TEST_SET)

    print("\nğŸ“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ (ê³ ì •):")
    print("-" * 40)
    for i, row in test_df.iterrows():
        print(f"  {i+1}. {row['question'][:60]}...")
    print("-" * 40)

    # 3. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("\n[1/3] RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    rag = AdvancedHybridRAG(
        top_k=25,            # ë” ë§ì€ í›„ë³´ ê²€ìƒ‰
        rerank_top_n=7,      # ë” ë§ì€ ë¬¸ì„œ ì‚¬ìš©
        bm25_weight=0.5,     # BM25 ê°€ì¤‘ì¹˜ ì¦ê°€ (í‚¤ì›Œë“œ ë§¤ì¹­ ê°•í™”)
        vector_weight=0.5,
        use_rewrite=True,
        use_decomposition=False,
        use_hyde=False,
        use_reranker=True,
    )

    # 4. í‰ê°€ ì‹¤í–‰
    print(f"\n[2/3] RAG í‰ê°€ ì§„í–‰ ì¤‘... ({len(test_df)}ê°œ ì§ˆë¬¸)")
    print("-" * 40)

    results = []
    scores = {"faithfulness": [], "relevancy": [], "correctness": []}

    for i, row in test_df.iterrows():
        question = row["question"]
        reference = row["reference"]

        print(f"\n  [{i+1}/{len(test_df)}] í‰ê°€ ì¤‘: {question[:40]}...")

        # RAGë¡œ ë‹µë³€ ìƒì„±
        transformed = rag.transform_query(question, verbose=False)
        documents = rag.hybrid_search(question, transformed, verbose=False)
        answer = rag.generate_answer(question, documents)
        context = " ".join([doc.page_content for doc in documents])

        # LLM-as-Judge í‰ê°€
        eval_result = evaluate_answer(question, answer, reference, context, llm)

        results.append({
            "question": question,
            "reference": reference,
            "rag_answer": answer[:500] + "..." if len(answer) > 500 else answer,
            "faithfulness": eval_result.get("faithfulness", 0),
            "relevancy": eval_result.get("relevancy", 0),
            "correctness": eval_result.get("correctness", 0),
            "comment": eval_result.get("comment", ""),
        })

        # ì ìˆ˜ ìˆ˜ì§‘
        for key in scores:
            scores[key].append(eval_result.get(key, 0))

        print(f"       â†’ F:{eval_result.get('faithfulness', 0)} R:{eval_result.get('relevancy', 0)} C:{eval_result.get('correctness', 0)}")
        print(f"       â†’ {eval_result.get('comment', '')[:50]}")

    # 5. ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    avg_scores = {key: sum(vals) / len(vals) if vals else 0 for key, vals in scores.items()}

    print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ë©”íŠ¸ë¦­             â”‚ í‰ê·     â”‚ í•´ì„                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Faithfulness       â”‚ {avg_scores['faithfulness']:.2f}    â”‚ ë¬¸ì„œ ì¶©ì‹¤ë„         â”‚
â”‚ Relevancy          â”‚ {avg_scores['relevancy']:.2f}    â”‚ ì§ˆë¬¸ ê´€ë ¨ì„±         â”‚
â”‚ Correctness        â”‚ {avg_scores['correctness']:.2f}    â”‚ ì •ë‹µ ì¼ì¹˜ë„         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ì „ì²´ í‰ê·           â”‚ {sum(avg_scores.values())/3:.2f}    â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ˆ ì ìˆ˜ ê°€ì´ë“œ:
  - 4.0 ì´ìƒ: ìš°ìˆ˜ (Production Ready)
  - 3.0-4.0: ì–‘í˜¸ (ê°œì„  ì—¬ì§€ ìˆìŒ)
  - 3.0 ë¯¸ë§Œ: ê°œì„  í•„ìš”
""")

    # 6. ìƒì„¸ ê²°ê³¼ ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
    csv_filename = f"evaluation_results_{timestamp}.csv"
    md_filename = f"evaluation_report_{timestamp}.md"

    results_df = pd.DataFrame(results)
    results_df.to_csv(csv_filename, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ '{csv_filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ì €ì¥
    markdown_report = generate_markdown_report(results, avg_scores, timestamp)
    with open(md_filename, "w", encoding="utf-8") as f:
        f.write(markdown_report)
    print(f"ğŸ“ í‰ê°€ ë³´ê³ ì„œê°€ '{md_filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 7. ê°œë³„ ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“‹ ê°œë³„ í‰ê°€ ê²°ê³¼")
    print("=" * 60)

    for i, r in enumerate(results, 1):
        print(f"""
[{i}] Q: {r['question'][:50]}...
    ì •ë‹µ: {r['reference'][:50]}...
    RAG: {r['rag_answer'][:50]}...
    ì ìˆ˜: F={r['faithfulness']} R={r['relevancy']} C={r['correctness']}
    í‰ê°€: {r['comment']}
""")

    print("=" * 60)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("=" * 60)

    return results_df


if __name__ == "__main__":
    run_evaluation()
