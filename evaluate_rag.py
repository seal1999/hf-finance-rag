"""
RAG ì‹œìŠ¤í…œ í‰ê°€ í”„ë¡œê·¸ë¨
1. ChromaDBì—ì„œ ë¬¸ì„œ ìƒ˜í”Œë§í•˜ì—¬ í•©ì„± í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± (LLM ê¸°ë°˜)
2. RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ ìƒì„±
3. LLM-as-Judgeë¡œ í‰ê°€ (Faithfulness, Relevancy, Correctness)
"""

import json
import pandas as pd
from datetime import datetime
from langchain_community.llms import Ollama
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from hybrid_rag_query import AdvancedHybridRAG

# ì„¤ì •
CHROMA_PERSIST_DIR = "./chroma_db"
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
COLLECTION_NAME = "finance_docs"
NUM_TEST_QUESTIONS = 10


def load_sample_documents(num_docs: int = 30):
    """ChromaDBì—ì„œ ìƒ˜í”Œ ë¬¸ì„œ ë¡œë“œ"""
    print(f"[1/4] ChromaDBì—ì„œ ìƒ˜í”Œ ë¬¸ì„œ ë¡œë“œ ì¤‘...")

    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )

    vectorstore = Chroma(
        collection_name=COLLECTION_NAME,
        embedding_function=embeddings,
        persist_directory=CHROMA_PERSIST_DIR,
    )

    # ë‹¤ì–‘í•œ ì¿¼ë¦¬ë¡œ ë¬¸ì„œ ìƒ˜í”Œë§
    sample_queries = [
        "ì‚¼ì„±ì „ì ì£¼ê°€",
        "ë°˜ë„ì²´ ì‹œì¥",
        "ì½”ìŠ¤í”¼ ì§€ìˆ˜",
        "ì™¸êµ­ì¸ íˆ¬ì",
        "í™˜ìœ¨ ì˜í–¥",
        "ê¸ˆë¦¬ ì¸ìƒ",
        "ì‹¤ì  ë°œí‘œ",
        "ë°°ë‹¹ê¸ˆ",
        "IPO ìƒì¥",
        "ê¸°ì—… ì¸ìˆ˜",
    ]

    all_docs = []
    seen_contents = set()

    for query in sample_queries:
        docs = vectorstore.similarity_search(query, k=5)
        for doc in docs:
            content_hash = hash(doc.page_content[:100])
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                all_docs.append(doc)
                if len(all_docs) >= num_docs:
                    break
        if len(all_docs) >= num_docs:
            break

    print(f"  - ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(all_docs)}")
    return all_docs


def generate_test_questions(docs, llm, num_questions: int = 10):
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸/ë‹µë³€ ìƒì„± (ê°œë³„ ìƒì„± ë°©ì‹)"""
    print(f"\n[2/4] í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ {num_questions}ê°œ ìƒì„± ì¤‘...")

    qa_pairs = []

    # ë¬¸ì„œë³„ë¡œ ì§ˆë¬¸ ìƒì„±
    for i, doc in enumerate(docs[:num_questions]):
        doc_content = doc.page_content[:1000]

        prompt = f"""ë‹¤ìŒ ê¸ˆìœµ ë¬¸ì„œë¥¼ ì½ê³  ì§ˆë¬¸ 1ê°œì™€ ì •ë‹µ 1ê°œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{doc_content}

ì§€ì¹¨:
- ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸
- ì§ˆë¬¸ì€ êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ
- ì •ë‹µì€ 1-2ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”:
ì§ˆë¬¸: [ì§ˆë¬¸ ë‚´ìš©]
ì •ë‹µ: [ì •ë‹µ ë‚´ìš©]"""

        try:
            response = llm.invoke(prompt)

            # íŒŒì‹±
            lines = response.strip().split("\n")
            question = ""
            answer = ""

            for line in lines:
                line = line.strip()
                if line.startswith("ì§ˆë¬¸:") or line.startswith("Question:"):
                    question = line.split(":", 1)[1].strip()
                elif line.startswith("ì •ë‹µ:") or line.startswith("Answer:"):
                    answer = line.split(":", 1)[1].strip()

            if question and answer:
                qa_pairs.append({"question": question, "answer": answer})
                print(f"  [{i+1}/{num_questions}] âœ“ {question[:40]}...")
            else:
                print(f"  [{i+1}/{num_questions}] âœ— íŒŒì‹± ì‹¤íŒ¨")

        except Exception as e:
            print(f"  [{i+1}/{num_questions}] âœ— ì˜¤ë¥˜: {e}")

        if len(qa_pairs) >= num_questions:
            break

    print(f"  - ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜: {len(qa_pairs)}")
    return qa_pairs


def evaluate_answer(question, answer, reference, context, llm):
    """LLM-as-Judgeë¡œ ë‹µë³€ í‰ê°€"""
    prompt = f"""ë‹¤ìŒ RAG ì‹œìŠ¤í…œì˜ ë‹µë³€ì„ í‰ê°€í•´ì£¼ì„¸ìš”.

## ì§ˆë¬¸
{question}

## ì •ë‹µ (ì°¸ì¡°)
{reference}

## RAG ì‹œìŠ¤í…œ ë‹µë³€
{answer}

## ê²€ìƒ‰ëœ ë¬¸ì„œ (Context)
{context[:1500]}...

## í‰ê°€ ê¸°ì¤€ (ê° 1-5ì )
1. **faithfulness**: ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œ(Context)ì— ì¶©ì‹¤í•œê°€? (í• ë£¨ì‹œë„¤ì´ì…˜ ì—†ëŠ”ê°€?)
2. **relevancy**: ë‹µë³€ì´ ì§ˆë¬¸ì˜ ì˜ë„ì— ì ì ˆíˆ ë¶€í•©í•˜ëŠ”ê°€?
3. **correctness**: ë‹µë³€ì´ ì •ë‹µ(ì°¸ì¡°)ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€?

## ì¶œë ¥ í˜•ì‹ (JSONë§Œ ì¶œë ¥)
{{"faithfulness": ì ìˆ˜, "relevancy": ì ìˆ˜, "correctness": ì ìˆ˜, "comment": "í•œì¤„í‰"}}

JSON:"""

    response = llm.invoke(prompt)

    try:
        result = response.strip()
        # JSON ê°ì²´ ì°¾ê¸°
        start_idx = result.find("{")
        end_idx = result.rfind("}") + 1
        if start_idx != -1 and end_idx > start_idx:
            result = result[start_idx:end_idx]
        return json.loads(result)
    except:
        return {"faithfulness": 0, "relevancy": 0, "correctness": 0, "comment": "íŒŒì‹± ì‹¤íŒ¨"}


def generate_markdown_report(results, avg_scores, timestamp):
    """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í‰ê°€ ë³´ê³ ì„œ ìƒì„±"""
    overall_avg = sum(avg_scores.values()) / 3

    report = f"""# RAG í‰ê°€ ê²°ê³¼ ë³´ê³ ì„œ

## ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½

| ë©”íŠ¸ë¦­ | í‰ê·  | í•´ì„ |
|--------|------|------|
| Faithfulness | {avg_scores['faithfulness']:.2f} | ë¬¸ì„œ ì¶©ì‹¤ë„ |
| Relevancy | {avg_scores['relevancy']:.2f} | ì§ˆë¬¸ ê´€ë ¨ì„± |
| Correctness | {avg_scores['correctness']:.2f} | ì •ë‹µ ì¼ì¹˜ë„ |
| **ì „ì²´ í‰ê· ** | **{overall_avg:.2f}** | |

### ì ìˆ˜ ê°€ì´ë“œ

- **4.0 ì´ìƒ**: ìš°ìˆ˜ (Production Ready)
- **3.0â€“4.0**: ì–‘í˜¸ (ê°œì„  ì—¬ì§€ ìˆìŒ)
- **3.0 ë¯¸ë§Œ**: ê°œì„  í•„ìš”

---

## ğŸ“‹ ê°œë³„ í‰ê°€ ê²°ê³¼

"""

    for i, r in enumerate(results, 1):
        report += f"""### [{i}] {r['question']}

- **ì •ë‹µ**: {r['reference']}
- **RAG ë‹µë³€**: {r['rag_answer']}
- **ì ìˆ˜**: F={r['faithfulness']}, R={r['relevancy']}, C={r['correctness']}
- **í‰ê°€**: {r['comment'] if r['comment'] else '(í‰ê°€ ì½”ë©˜íŠ¸ ì—†ìŒ)'}

---

"""

    report += """## ë©”íŠ¸ë¦­ ì„¤ëª…

- **F (Faithfulness)**: RAG ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€ (í™˜ê° ì—¬ë¶€)
- **R (Relevancy)**: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€
- **C (Correctness)**: ë‹µë³€ì´ ì°¸ì¡° ì •ë‹µê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€

*ìƒì„¸ ìˆ˜ì¹˜ ê²°ê³¼ëŠ” `evaluation_results_{timestamp}.csv`ì— ì €ì¥ë¨.*
"""

    return report.replace("{timestamp}", timestamp)


def run_evaluation():
    """ì „ì²´ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    print("=" * 60)
    print("ğŸ§ª RAG ì‹œìŠ¤í…œ í‰ê°€ ì‹œì‘")
    print(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    # 1. LLM ì´ˆê¸°í™”
    print("\n[0/4] LLM ì´ˆê¸°í™” ì¤‘...")
    llm = Ollama(model="llama3.2", temperature=0.3)

    # 2. ìƒ˜í”Œ ë¬¸ì„œ ë¡œë“œ
    docs = load_sample_documents(num_docs=30)

    # 3. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ìƒì„±
    qa_pairs = generate_test_questions(docs, llm, num_questions=NUM_TEST_QUESTIONS)
    if not qa_pairs:
        print("âŒ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")
        return

    # DataFrameìœ¼ë¡œ ë³€í™˜
    test_df = pd.DataFrame(qa_pairs)
    test_df.columns = ["question", "reference"]

    print("\nğŸ“ ìƒì„±ëœ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸:")
    print("-" * 40)
    for i, row in test_df.iterrows():
        print(f"  {i+1}. {row['question'][:60]}...")
    print("-" * 40)

    # 4. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("\n[3/4] RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    rag = AdvancedHybridRAG(
        top_k=15,
        rerank_top_n=5,
        bm25_weight=0.3,
        vector_weight=0.7,
        use_rewrite=True,
        use_decomposition=False,
        use_hyde=False,
        use_reranker=True,
    )

    # 5. í‰ê°€ ì‹¤í–‰
    print(f"\n[4/4] RAG í‰ê°€ ì§„í–‰ ì¤‘... ({len(test_df)}ê°œ ì§ˆë¬¸)")
    print("-" * 40)

    results = []
    scores = {"faithfulness": [], "relevancy": [], "correctness": []}

    for i, row in test_df.iterrows():
        question = row["question"]
        reference = row["reference"]

        print(f"\n  [{i+1}/{len(test_df)}] í‰ê°€ ì¤‘: {question[:40]}...")

        # RAGë¡œ ë‹µë³€ ìƒì„±
        transformed = rag.transform_query(question, verbose=False)
        documents = rag.hybrid_search(question, transformed, verbose=False)
        answer = rag.generate_answer(question, documents)
        context = " ".join([doc.page_content for doc in documents])

        # LLM-as-Judge í‰ê°€
        eval_result = evaluate_answer(question, answer, reference, context, llm)

        results.append({
            "question": question,
            "reference": reference,
            "rag_answer": answer[:300] + "..." if len(answer) > 300 else answer,
            "faithfulness": eval_result.get("faithfulness", 0),
            "relevancy": eval_result.get("relevancy", 0),
            "correctness": eval_result.get("correctness", 0),
            "comment": eval_result.get("comment", ""),
        })

        # ì ìˆ˜ ìˆ˜ì§‘
        for key in scores:
            scores[key].append(eval_result.get(key, 0))

        print(f"       â†’ F:{eval_result.get('faithfulness', 0)} R:{eval_result.get('relevancy', 0)} C:{eval_result.get('correctness', 0)}")
        print(f"       â†’ {eval_result.get('comment', '')[:50]}")

    # 6. ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    avg_scores = {key: sum(vals) / len(vals) if vals else 0 for key, vals in scores.items()}

    print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ë©”íŠ¸ë¦­             â”‚ í‰ê·     â”‚ í•´ì„                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Faithfulness       â”‚ {avg_scores['faithfulness']:.2f}    â”‚ ë¬¸ì„œ ì¶©ì‹¤ë„         â”‚
â”‚ Relevancy          â”‚ {avg_scores['relevancy']:.2f}    â”‚ ì§ˆë¬¸ ê´€ë ¨ì„±         â”‚
â”‚ Correctness        â”‚ {avg_scores['correctness']:.2f}    â”‚ ì •ë‹µ ì¼ì¹˜ë„         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ì „ì²´ í‰ê·           â”‚ {sum(avg_scores.values())/3:.2f}    â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ˆ ì ìˆ˜ ê°€ì´ë“œ:
  - 4.0 ì´ìƒ: ìš°ìˆ˜ (Production Ready)
  - 3.0-4.0: ì–‘í˜¸ (ê°œì„  ì—¬ì§€ ìˆìŒ)
  - 3.0 ë¯¸ë§Œ: ê°œì„  í•„ìš”
""")

    # 7. ìƒì„¸ ê²°ê³¼ ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
    csv_filename = f"evaluation_results_{timestamp}.csv"
    md_filename = f"evaluation_report_{timestamp}.md"

    results_df = pd.DataFrame(results)
    results_df.to_csv(csv_filename, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ '{csv_filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ì €ì¥
    markdown_report = generate_markdown_report(results, avg_scores, timestamp)
    with open(md_filename, "w", encoding="utf-8") as f:
        f.write(markdown_report)
    print(f"ğŸ“ í‰ê°€ ë³´ê³ ì„œê°€ '{md_filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 8. ê°œë³„ ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“‹ ê°œë³„ í‰ê°€ ê²°ê³¼")
    print("=" * 60)

    for i, r in enumerate(results, 1):
        print(f"""
[{i}] Q: {r['question'][:50]}...
    ì •ë‹µ: {r['reference'][:50]}...
    RAG: {r['rag_answer'][:50]}...
    ì ìˆ˜: F={r['faithfulness']} R={r['relevancy']} C={r['correctness']}
    í‰ê°€: {r['comment']}
""")

    print("=" * 60)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("=" * 60)

    return results_df


if __name__ == "__main__":
    run_evaluation()
